\def\paperversiondraft{draft}
\def\paperversionnormal{normal}

% If the paper version is set to 'normal' mode keep it,
% otherwise set it to 'draft' mode.
\ifx\paperversion\paperversionnormal
\else
  \def\paperversion{draft}
\fi

\documentclass[a4paper]{article}

\usepackage{colortbl}

% 'draftonly' environment
\usepackage{environ}
\ifx\paperversion\paperversiondraft
\newenvironment{draftonly}{}{}
\else
\NewEnviron{draftonly}{}
\fi

% Most PL conferences are edited by conference-publishing.com. Follow their
% advice to add the following packages.
%
% The first enables the use of UTF-8 as character encoding, which is the
% standard nowadays. The second ensures the use of font encodings that support
% accented characters etc. (Why should I use this?). The mictotype package
% enables certain features 'to­wards ty­po­graph­i­cal per­fec­tion
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}

\usepackage{xargs}
\usepackage{lipsum}
\usepackage{xparse}
\usepackage{xifthen, xstring}
\usepackage{xspace}
\usepackage{marginnote}
\usepackage{etoolbox}
\usepackage[acronym,shortcuts]{glossaries}

\input{tex/setup.tex}
% \input{tex/acm.tex}

\usemintedstyle{colorful}

% Newer versions of minted require the 'customlexer' argument for custom lexers
% whereas older versions require the '-x' to be passed via the command line.
\makeatletter
\ifcsdef{minted@optlistcl@quote}
{
\newminted[mlir]{tools/MLIRLexer.py:MLIRLexerOnlyOps}{customlexer, mathescape}
\newminted[xdsl]{tools/MLIRLexer.py:MLIRLexer}{customlexer, mathescape, style=murphy}
\newminted[lean4]{tools/Lean4Lexer.py:Lean4Lexer}{customlexer, mathescape}
}
{
\newminted[mlir]{tools/MLIRLexer.py:MLIRLexerOnlyOps -x}{mathescape}
\newminted[xdsl]{tools/MLIRLexer.py:MLIRLexer -x}{mathescape, style=murphy}
\newminted[lean4]{tools/Lean4Lexer.py:Lean4Lexer -x}{mathescape}
}
\makeatother

% We use the following color scheme
% 
% This scheme is both print-friendly and colorblind safe for
% up to four colors (including the red tones makes it not
% colorblind safe any more)
%
% https://colorbrewer2.org/#type=qualitative&scheme=Paired&n=4

\definecolor{pairedNegOneLightGray}{HTML}{cacaca}
\definecolor{pairedNegTwoDarkGray}{HTML}{827b7b}
\definecolor{pairedOneLightBlue}{HTML}{a6cee3}
\definecolor{pairedTwoDarkBlue}{HTML}{1f78b4}
\definecolor{pairedThreeLightGreen}{HTML}{b2df8a}
\definecolor{pairedFourDarkGreen}{HTML}{33a02c}
\definecolor{pairedFiveLightRed}{HTML}{fb9a99}
\definecolor{pairedSixDarkRed}{HTML}{e31a1c}

\createtodoauthor{grosser}{pairedOneLightBlue}
\createtodoauthor{authorTwo}{pairedTwoDarkBlue}
\createtodoauthor{authorThree}{pairedThreeLightGreen}
\createtodoauthor{authorFour}{pairedFourDarkGreen}
\createtodoauthor{authorFive}{pairedFiveLightRed}
\createtodoauthor{authorSix}{pairedSixDarkRed}

\newacronym{ir}{IR}{Intermediate Representation}

\graphicspath{{./images/}}

% Define macros that are used in this paper
%
% We require all macros to end with a delimiter (by default {}) to enusure
% that LaTeX adds whitespace correctly.
\makeatletter
\newcommand\requiredelimiter[2][########]{%
  \ifdefined#2%
    \def\@temp{\def#2#1}%
    \expandafter\@temp\expandafter{#2}%
  \else
    \@latex@error{\noexpand#2undefined}\@ehc
  \fi
}
\@onlypreamble\requiredelimiter
\makeatother

\newcommand\newdelimitedcommand[2]{
\expandafter\newcommand\csname #1\endcsname{#2}
\expandafter\requiredelimiter
\csname #1 \endcsname
}

\newdelimitedcommand{leanmlir}{LeanMLIR}

\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\usepackage[verbose]{newunicodechar}
\newunicodechar{₁}{\ensuremath{_1}}
\newunicodechar{₂}{\ensuremath{_2}}
\newunicodechar{∀}{\ensuremath{\forall}}
\newunicodechar{α}{\ensuremath{\alpha}}
\newunicodechar{β}{\ensuremath{\beta}}

% \circled command to print a colored circle.
% \circled{1} pretty-prints "(1)"
% This is useful to refer to labels that are embedded within figures.
\DeclareRobustCommand{\circled}[2][]{%
    \ifthenelse{\isempty{#1}}%
        {\circledbase{pairedOneLightBlue}{#2}}%
        {\autoref{#1}: \hyperref[#1]{\circledbase{pairedOneLightBlue}{#2}}}%
}

% listings don't write "Listing" in autoref without this.
\providecommand*{\listingautorefname}{Listing}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\begin{document}

%% Title information
% \title{First Year Report}       %% [Short Title] is optional;
                                      %% when present, will be used in
                                      %% header instead of Full Title.
\title{Towards a framework for reasoning about stateful compiler IRs 
using coinductive reasoning.}                                      
\subtitle{First Year Report}

%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.
\author{Alex Keizer}
% \authornote{ack55}          %% \authornote is optional;
                                      %% can be repeated if necessary
% \orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
% \affiliation{
%   \position{Position1}
%   \department{Department1}              %% \department is recommended
%   \institution{Institution1}            %% \institution is required
%   \streetaddress{Street1 Address1}
%   \city{City1}
%   \state{State1}
%   \postcode{Post-Code1}
%   \country{Country1}
% }
\email{ack55@cam.ac.uk}          %% \email is recommended

%        %% \email is recommended

% \begin{abstract}
% % An abstract should consist of six main sentences:
% %  1. Introduction. In one sentence, what’s the topic?
% %  2. State the problem you tackle.
% %  3. Summarize (in one sentence) why nobody else has adequately answered the research question yet.
% %  4. Explain, in one sentence, how you tackled the research question.
% %  5. In one sentence, how did you go about doing the research that follows from your big idea.
% %  6. As a single sentence, what’s the key impact of your research?

% % (http://www.easterbrook.ca/steve/2010/01/how-to-write-a-scientific-abstract-in-six-easy-steps/)

% \lipsum[1]
% \end{abstract}


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

Formal verification is crucial in ensuring the reliability and
correctness of software systems. In the context of compiler design, this
importance is magnified as compilers are responsible for translating
high-level code into machine-executable instructions. Reasoning about
compiler optimizations is notoriously difficult, and even minor bugs in
widely used compilers can lead to subtle, hard-to-debug miscompilations
with far-reaching consequences. This challenge makes the formal
verification of compiler intermediate representations (IRs) an
especially compelling topic, as it promises to enhance the dependability
of the entire software development lifecycle.

Now, formal verification has been done before. Most notable is the
CompCert project: a fully verified end-to-end C compiler.
{[}{[}@leroyCompCertFormallyVerified{]}{]} Yet, the vast majority of
code today is still being compiled with non-verified compilers. We ask,
thus, how do we formally verify not \emph{a} compiler, but, how do we
scale formal verification to \emph{mainstream} compilers.

Take, for example, LLVM, a widely-used open-source compiler that is used
to compile, e.g., C, C++, Fortran, Rust, and Swift. Verifying the entire
LLVM codebase, comprising of a few million lines of C++ code, is
infeasible. Instead, we follow the same approach as Lopez et al and
focus on verifying that individual rewrites on a compiler IR preserve
semantics. {[}{[}@lopesAlive2BoundedTranslation2021{]}{]}

Although LLVM is indeed used by the standard compilers for Rust and
Swift, both also use other, language-specific, IRs to be able to do
analyses and optimizations at a higher level of abstraction. In
response, the MLIR (Multi-Level Intermediate Representation\footnote{Multi-Level
  Intermediate Representation is the \emph{official} expansion of MLIR,
  but other expansions are used by the community as well}) project has
been created as a framework for creating domain-specific IRs (generally
called a \emph{dialect}) at various levels of abstraction.

During the first year of my PhD studies, my research has focused on the
LeanMLIR project, a framework for defining semantics for MLIR dialects
within the Lean theorem prover. A central aspect of this work has been
modelling MLIR dialects with completely pure semantics, exploiting the
higher-level nature of MLIR dialect, where referential transparency is
encouraged.

We need the full power of a theorem prover to be able to capture the
full extensible nature of MLIR. Still, to recover the ease-of-use that
SMT-solver-based tools provide, we've also been investing in building
solid automation for bitvector proofs in collaboration with the Lean
community.

In the next two years, I'd like to look into a feature of MLIR called
\emph{Graph Regions}. Specific dialects may use such regions to allow a
variable to be used \emph{before} it is defined --- hence, turning the
def-use chain into a general graph. Generally, the semantics of such
dialects are statefull, and cycles are broken by looking at the value of
a variable at a previous state. For example, graph regions are used by
dialects that model hardware {[}{[}@eldridgeMLIRHardwareCompiler{]}{]},
where variables model wires and cycles represent physical cycles in the
circuit.

Since the exact semantics of graph regions may differ between dialects,
we need some general framework to reason about stateful behaviours in
Lean. I propose that coinductive types are this framework, and would
like to continue my earlier work on supporting coinductive reasoning in
Lean, with the aim of using them to model graph regions in LeanMLIR.

In fact, coinductive types are useful beyond graph regions. Since
coinduction models infinite behaviours, it is also a good way to model
non-termination without giving up on computability of a dialect's
semantics, thus addressing a current short-coming of LeanMLIR.

\hypertarget{related-work}{%
\section{Related work}\label{related-work}}

First, we'll review the literature on verified compilation, which
naturally leads us to the literature on coinductive types.

\hypertarget{verified-compilation}{%
\subsection{Verified Compilation}\label{verified-compilation}}

{[}{[}@leroyCompCertFormallyVerified\textbar CompCert{]}{]} is the
seminal work when it comes to verified compilation: an end-to-end
verified to not miscompile compiler for the C language. However, exactly
because it's an end-to-end verified monolith, CompCert is not
competitive compared to traditional non-verified compilers. Outside of
certain high-risk domains, where traditionally optimizations would be
turned off entirely {[}{[}@kastnerCompCertPracticalExperience{]}{]}, the
vast majority of C code is thus still compiled with non-verified
compilers, which studies have shown to be prone to miscompilations
{[}{[}@yangFindingUnderstandingBugs2011{]}{]}.

Lopes et al.~propose \emph{translation validation} as a way to scale
formal verification. {[}{[}@lopesAlive2BoundedTranslation2021{]}{]}
Their implementation, called Alive, has found wide adoption in the
development of LLVM. Given a declarative description of an optimization
as a source and target pattern in LLVM IR, the tool uses an SMT solver
to automatically determine whether the optimized pattern exhibits only
behaviours of the original program.

Alive is able to leverate SMT solvers because the semantics of LLVM IR
are given in terms of bitvectors, and SMT solvers understand the
bitvector domain well. However, the semantics of more high-level
compiler IRs, for which MLIR was created, can involve complex
mathematical objects, which today's SMT solvers are not able to prove
properties about. For example, the semantics for the ``Poly'' dialect
\footnote{https://homomorphicencryption.org/} are generally given in
terms of finitely-presented commutative rings. Furthermore, MLIR is, by
design, extensible, so a translation validation tool for MLIR also needs
to be easily extensible to cover new dialects.

Our group has been developing
{[}\protect\hyperlink{leanmlir}{LeanMLIR}{]}, a framework for giving
semantics to MLIR-style compiler IRs in the Lean theorem prover
{[}{[}@demouraLeanTheoremProver2015{]}{]}. As a theorem prover, Lean is
much more expressible than SMT solvers. Among theorem provers, Lean is
particularly suited for this purpose because of it's community of
mathematicians developing a comprehensive mathematical library
{[}{[}@themathlibcommunityLeanMathematicalLibrary2020{]}{]}, and because
of it's highly extensive syntax and meta-programming capabilities
{[}{[}@ullrichNotationsHygienicMacro2022{]}{]}{[}{[}@paulinoMetaprogrammingLean{]}{]}.

However, using a theorem prover means we don't get as much automation
out of the box, and one area where Lean is still lacking is in its lack
of support for coinductive types.

\hypertarget{coinductive-types}{%
\subsection{Coinductive Types}\label{coinductive-types}}

Lean, like most theorem provers, is built with around a native concept
of inductive types. This is no surprise, given that most mathematical
constructions are inductive in nature. Coinductive constructions, in
contrast, are less common in general mathematics, but, they are crucial
in computer science.

For us, the main difference of interest is that inductive datastructures
are finite, while coinductive datastructures capture infinite
behaviours.
{[}{[}@sangiorgiIntroductionBisimulationCoinduction2011{]}{]} In
particular, since Lean requires us to prove that recursive functions are
terminating, we need corecursion to correctly model things like infinite
loops and other non-terminating behaviours that are common in compiler
IRs.

For example, Xia et al.~used interaction trees, a coinductive type, in
their Coq formalization of recursive and impure programs.
{[}{[}@xiaInteractionTreesRepresenting2020{]}{]} Similarly, MLIR has an
``fsm'' dialect \footnote{https://circt.llvm.org/docs/Dialects/FSM/}
centred on finite state machines. Such automata are understood to be
coinductive in nature, so mechanizing the semanantics of this dialect in
Lean will likely require coinduction.

Consequently, we need a user-friendly way to define coinductive
structures and to formalize coinductive proof methods in Lean. Key prior
work in this area include my own master's thesis
{[}{[}@keizerImplementingDefinitionalCo{]}{]}, which is based on earlier
work by Avigad et al that models coinductive as quotients of polynomial
functors (QPFs){[}{[}@avigadDataTypesQuotients2019{]}{]}.

Crucially, this prior work builds coinductive types using constructions
already supported by Lean today. This allows us to provide coinductive
types as a \emph{library}, instead of having to modify the logical
system itself to support coinduction natively --- an approach that is
greatly aided by Lean's excellent meta-programming capabilities
mentioned previously.

When looking at coinductive types in other theorem provers, we
distinguish between languages like Isabelle
{[}{[}@traytelCategoryTheoryBased{]}{]}, which follow the same
coinduction-as-a-library approach, and languages like Coq
{[}{[}@gimenezTutorialRecursiveTypes1998{]}{]}{[}{[}@gimenezApplicationCoinductiveTypes1996{]}{]}
or Agda which have modified their trusted kernels to support
coinduction.

Isabelle, in particular, is relevant because it's construction of
coinductive types, in terms of \emph{bounded natural functors} is
closely related to our construction in terms of QPFs.
{[}{[}@furerQuotientsBoundedNatural2022{]}{]} There are still new
challenges, however, because Lean's dependent types mean we have to
worry about the distinction between definitional equality (which is a
decidable, but weaker notion of equality) and propositional equality
(which captures more things that are intuitively equal, but requires
proof). Isabelle, in contrast, uses a weaker logical system where all
equalities are decidable.

In contrast, the coinduction-in-the-kernel approach allows us to modify
the kernel to understand coinduction and ensure the kernel recognizes
the desired definitonal equalities. However, modifying the trusted
kernel carries a large burden of proof, since the changes could
compromise the logical soundness of the entire system. In fact, the
original implementation of coinduction, called \emph{positive}
coinduction, is nowadays discouraged in favour of an alternative
implementation, since positive coinduction breaks the subject reduction
property \footnote{ The subject reduction property states that reducing
  a program does not change its type}{[}{[}@sozeauCorrectCompleteType{]}{]}.
Our coinductives-as-a-library approach requires no new axioms nor
changes to the kernel, and is thus guaranteed not to change any meta
properties of Lean.

\hypertarget{progress-report}{%
\section{Progress Report}\label{progress-report}}

The bulk of my time has been spent on developing, in collaboration with
other members of our research group, the LeanMLIR framework, culminating
in our ITP 2024 paper \emph{``Verifying Peephole Rewriting In SSA
Compiler IRs''}. As part of this development, we

\hypertarget{lean-mlir}{%
\subsection{Lean MLIR}\label{lean-mlir}}

LeanMLIR is a framework for giving semantics to MLIR dialects in Lean.
On top of this framework, we've built a peephole rewriter, and proved
that if a rewrite locally preserves semantics, then applying this
rewrite in the context of a larger program will preserve the semantics
of the program as a whole.

However, we've focused on dialects with pure (i.e., side-effect free)
semantics. Even though we \emph{are} able to verify a lot of interesting
rewrites in high-level MLIR dialects, we do need to be able to reason
about side-effect if we want to capture the low-level dialects as well,
as a reviewer of the paper rightly pointed out.

To this end, we've generalised the framework so that a dialect's
semantics may be given in a user-defined monad, and proven that applying
a semantics-preserving pure rewrite (i.e., where the operations being
rewritten are all side-effect free) inside of a side-effecting program
preserves semantics of the program as a whole. However, this
generalisation still has a lot of rough edges, and further work is
needed before we can claim our framework is versatile enough to deal
with side-effects cleanly.

\hypertarget{bitvectors}{%
\subsection{Bitvectors}\label{bitvectors}}

As mentioned, a big reason for us to use Lean is it's community. As
such, I've been heavily involved in the redesign of Lean's API for
bitvectors, and the move of bitvectors from Mathlib (the mathematical
library previously mentioned) to become part of Lean's core library
(i.e., the library that is by default included in every Lean program).
By ensuring our representation of bitvectors is aligned with the rest of
the community, we ensure we can easily reap the benefits of, e.g.,
tactics to automate bitvector proofs made by others.

One such automation effort is the LeanSAT\footnote{https://github.com/leanprover/leansat}
library, which aims to automate proofs about bitvectors of bounded
widths, by implementing a verified bitblaster and SAT sovler in Lean.
Siddharth (another PhD student in our group) and I have been
contributing to this project by formally proving necessary facts in the
core library.

This automation is crucial, because with it we can provide push-button
automation for non-expert users on the same level as Alive, for dialects
whose semantics use bitvectors, while also still having the full
expressivity needed for dialects that use more complex mathematical
objects. Thus, addressing the main drawback of using a theorem prover.

\hypertarget{future-plans}{%
\section{Future plans}\label{future-plans}}

\hypertarget{bitvectors-1}{%
\subsection{Bitvectors}\label{bitvectors-1}}

\begin{quote}
June '24 - January '25
\end{quote}

As mentioned in the progress report, I'm currently contributing to a
verified bitblasting procedure in Lean, called LeanSAT. Once this is
implemented, I plan to write a paper contrasting this verified
bitblaster and SAT solver with popular non-verified SMT solvers like Z3,
to investigate what exactly the cost is of the extra assurance that a
verified implementation gives. This is particularly relevant, since,
just like compilers, SMT solvers are highly complicated pieces of
software in which bugs are known to exist
{[}{[}@brummayerFuzzingDeltadebuggingSMT2009{]}{]}.

\hypertarget{arbitrary-widths}{%
\subsubsection{Arbitrary widths}\label{arbitrary-widths}}

LeanSAT is, however, not a silver bullet. Just like SMT solvers, it can
only reason about bitvector expressions where the width of the
bitvectors involved is bounded. One of the topics I'd like to explore
more is the connection between bitvectors and 2-adic numbers (which are
infinite and thus coinductive in nature!) to build a decision procedure
which can decide equality of a certain (limited) class of
arbitrary-width bitvector expressions.

Beyond being more satisfying, this arbitrary-width procedure is likely
more performant than SAT solving, and thus relevant even if the end-user
isn't necessarily interested in proving their property for all possible
widths.

\hypertarget{amazon-internship}{%
\subsubsection{Amazon Internship}\label{amazon-internship}}

\begin{quote}
July '24 - November '24
\end{quote}

Furthermore, I plan to continue this work on bitvector automation in
Lean during my internship at Amazon \footnote{ Assuming I'll get
  permission to work away, of course}. They are particularly interested
in automatically reasoning about ARM assembly code, which closely aligns
with our goal of automating bitvector reasoning to then use for
verifying (low-level) MLIR dialects.

\hypertarget{coinduction}{%
\subsection{Coinduction}\label{coinduction}}

The next big theme to tackle will be to make the Lean library for
coinductive types ready for serious use, which mainly involves
high-level support for corecursive functions.

\hypertarget{productivity-checking}{%
\subsubsection{Productivity Checking}\label{productivity-checking}}

\begin{quote}
June '24 - Aug '24 (Supervising Internship)
\end{quote}

Currently, the \texttt{QpfTypes} framework is able to compile a
high-level description of a coinductive type into a construction in
terms of concepts Lean already supports. As part of this construction, a
\emph{corecursion principle} is generated. For example, the corecursion
principle for \texttt{Stream\ \textbackslash{}alpha}, the type of
infinite lists (i.e., streams) over elements of type
\texttt{\textbackslash{}alpha}, is as follows:

\begin{verbatim}
Stream.corec : (\beta -> \alpha \times \beta) -> \beta -> Stream \a
\end{verbatim}

While this corecursion principle is in theory all that's needed to
define functions over streams, it's not directly usable in practice.
Hence, the first subgoal will be to support a high-level way to define
corecursive functions, following the idiomatic way of defining inductive
functions in terms of pattern matching, rather than recursion
principles, which is then compiled into an application of this
corecursion princple. This will involve some software engineering,
dealing with Lean's extensible syntax, plus a good amount of research,
to know which functions are \emph{productive} in our formalization, and
to automatically recognize (a well-defined sub-class of) these
productive corecursive functions.

Productivity in this context is the dual of termination in recursive
contexts; a corecursive function is said to be productive if, at all
times, it will produce the next element of our infinite corecursive
structure in finite time. Ensuring all corecursive functions are
productive is paramount to our logical system remaining consistent. In
fact, because of our coinductives-as-a-library approach we are unable to
define corecursive functions without proving they are productive, and
conversely, if we can get the system to accept a corecursive function,
we know that allowing this function must be productive, and allowing it
is sound.

I will be a co-supervising an undergraduate student over the summer who
will be implementing support for corecursive functions using a
\emph{guarded} productivity checker. That is, this first implementation
will only allow only functions where each corecursive call occurs behind
a constructor. This check is known to incomplete, and more complicated
schemes have been studied
({[}{[}@blanchetteFriendsBenefitsImplementing2017{]}{]}), but a
guardedness easier to implement and will most likely be sufficient for
now --- the initial implementation of corecursion in Coq also allowed
only guarded corecursion {[}{[}@gimenezTutorialRecursiveTypes1998{]}{]}.
Of course, if there is still time left in the internship after this goal
has been reached, we will attempt to implement more complicated schemes
as well.

\hypertarget{runtime-behaviour}{%
\subsubsection{Runtime behaviour}\label{runtime-behaviour}}

\begin{quote}
Jan '25 - May '25
\end{quote}

A cornerstone of the coinduction-as-a-library approach is that we're
limited to whatever concepts our theorem prover already supports. The
QPF construction starts by realizing that an element of an inductive
type is basically a (finite) tree, while an element of a coinductive
type is a non-wellfounded (i.e., possibly infinite) tree.

Finite trees are easily encoded in Lean, but representing a finite tree
is a bit more tricky. The current construction works by representing an
non-wellfounded tree as a function from a natural number \(n\) to the
approximation of the non-wellfounded tree up to a maximal depth of
\(n\), together with a proof that each subsequent approximation agrees
with the previous approximations up to depth \(n-1\).

This construction is mathematically elegant, but seems to lead to
unexpected inefficient computational behaviour, as shown by some early
experimentation by myself. I plan to investigate this more thoroughly,
but I strongly suspect this behaviour comes from the impedance mismatch
between the construction of infinite trees as just described, and the
fact that applications of corecursion principles are essentially state
machines. I would thus like to explore the possibility of building the
QPF constructions on a representation of non-wellfounded trees in terms
of state machines (or, more generally, coalgebras).

\hypertarget{leanmlir}{%
\subsection{LeanMLIR}\label{leanmlir}}

Finally, we return to the main objective: formalizing compiler IRs.

\hypertarget{non-termination}{%
\subsubsection{Non-termination}\label{non-termination}}

\begin{quote}
Apr '25 - June '25
\end{quote}

In the status report, we discussed how the framework was generalized to
work in arbitrary monads. But, to actually define the semantics of an
MLIR dialect which allows for infinite loops, we need a concrete monad
that can support this behaviour.

We could, of course, model non-termination using the \texttt{Option}
monad, where divergence is simply modelled as \texttt{Option.none}. Such
semantics are necessarily non-computable, losing the very nice property
that the formalization of a dialect in LeanMLIR also gives a reference
implementation for an interpreter.

Therefore, I'd like to use my library for coinductive types to define a
coinductive monad which supports computable infinite behaviours, drawing
inspiration from the interaction tree library for coq.
{[}{[}@xiaInteractionTreesRepresenting2020{]}{]} This addresses a
weakness in LeanMLIR while simultaneously giving a nice case study for
the usability of the coinduction library.

\hypertarget{graph-regions-parametric-coinduction}{%
\subsubsection{Graph Regions \& Parametric
Coinduction}\label{graph-regions-parametric-coinduction}}

\begin{quote}
June '25 - Sep '25
\end{quote}

A \emph{Graph Region} in MLIR is, just like a regular region, a sequence
of instructions with their results bound to variables. However, unlike
normal regions, in a graph region the \emph{use} of a variable may
precede it's definition. For example

\begin{verbatim}
^bb(%x : i32)
  %y = comb.and %x %y : (i32, i32) -> (i32)
\end{verbatim}

Here, \texttt{y} is obtained as the bitwise and of the argument
\texttt{x} and \texttt{y} itself. As a program, this doesn't make much
sense, but this style of definition is common in hardware
representations, where this is interpreted as: ``the value of \texttt{y}
in the \emph{current} clock cycle is computed by taking the bitwise and
of the argument \texttt{x} and the \emph{previous} value of \texttt{y}''
\footnote{This example is simplified, in reality you'd have to specify a
  register instruction to control when a value is stored across clock
  cycles. See https://circt.llvm.org/ for more details}. To formally
model this, we could denote variables as infinite streams of bitvectors,
which gives us the value at every possible future clock cycle. Another
example of a dialect with graph regions is the FSM, or Finite State
Machine, dialect.

Graph regions are thus inherently stateful, and modelling them is a
particularly good use-case for coinductive types and coinductive
reasoning. To facilitate easy verification of coinductive properties in
this setting, I would like to investigate supporting parametrized
coinductive proofs {[}{[}@hurPowerParameterizationCoinductive{]}{]} in
the coinductive types framework.

\hypertarget{stretch-goal-iris}{%
\subsection{Stretch Goal: Iris}\label{stretch-goal-iris}}

\begin{quote}
{[}!TODO{]} Talk about how denotational semantics are cool, and how the
aforementioned improvements will let us define dialects with
side-effects, but reasoning about them will still be a bit painful. We'd
really like to reason with program logics instead, in particular, iris:
{[}{[}@jungIrisMonoidsInvariants2015{]}{]}{[}{[}@jungHigherorderGhostState2016{]}{]}
\end{quote}


% \input{y1review.tex}

% \begin{figure}
% % Link to figure
% %
% % https://docs.google.com/drawings/d/1juKp43D3rLC-luBQPwQZ_wCnDK2S_6C1k6USV0wKE0g/edit?usp=sharing
% \includegraphics[width=\columnwidth]{overview.pdf}
% \caption{Our key idea visualized}
% \grosser{Replace this figure with your own drawing.}
% \end{figure}









%% Bibliography
\bibliography{references}


%% Appendix
% Move \cleardouble and \appendix out of `draftonly` if you are using the
% appendix
\begin{draftonly}
\cleardoublepage

\appendix
\section{Formatting and Writing Guidelines}

These formatting guidelines aim to standardize our writing. They ensure that
papers with multiple authors have a consistent look and that commonly occurring
items are formatted in ways that are known to work well.

\subsection{Figures}
\label{appendix:figures}

\paragraph{Referencing Figures} When referencing figures from the text we
ensure the following:
\begin{description}
      \item [All figures are referenced] A paper with un-referenced figures
	      appears incomplete.
      \item [References to figures are brief and easy to skip]~\\
                We minimize the number of words needed to refer to a figure. Reducing
                the number of non-information-carrying words directly increases
		the density of interesting content. When skipping references
		becomes easy, reading quickly while ignoring figures remains a
		smooth experience. The best and briefest reference to a figure
		is a link in parenthesis that is added after the subject
		representing the content depicted in a figure:\\
		{\color{pairedTwoDarkBlue}\textit{Figure
		X shows the design of A, which consists of ...}}\\
		$\to$ {\color{pairedFourDarkGreen}
		\textit{The design of A (Figure X) consists of ...}}
      \item [The text is always self-contained without figures] ~\\ The reader
                should be able to read the text without ever looking at any
                figure. They should still understand the text and get the key
		message of each figure directly from the text. By not forcing
		the reader to analyze a figure while reading, we increase
		readability as the reader can continue reading without having
		to skip between text and figures. Such writing style also helps
		to guide the thoughts of the reader, who can (for a moment)
		trust our summary of the figure and does not need to develop
		their own interpretation on-the-fly, a task which often yields
		results that do not fit the flow of our exposition. Readers
		typically only feel that their reading is interrupted if there
		is no explanation of a figure at all. Hence, we do not need to
		discuss all details of a figure, but half a sentence that explains the
		core idea is typically sufficient for a reader to continue
		reading.  By making
		our text self-contained even when ignoring figures the reader
		experiences a smooth and uninterrupted reading experience.\\
		{\color{pairedTwoDarkBlue}
		\textit{The speedups are presented in Figure X. < a new topic> }}\\
		$\to$ {\color{pairedFourDarkGreen}\textit{Our approach outperforms the state of the art
		XXX-library (Figure 3) demonstrating more than 4x speedup on
		test case 1 and 2 and a geometric mean speedup of 1.5x over all
		20 test cases.}}
\end{description}
We reference figures in text using
\texttt{\symbol{92}autoref\{fig:speedup\}} for a figure with label
\texttt{fig:speedup}.  The use of autoref ensures that all references
to figures are formatted consistently, e.g. as \autoref{fig:speedup}.

\paragraph{Color Scheme} 

In Figures we use a color scheme that is print-friendly and also visible
with red-green blindness. The following colors are all print-friendly
and red-green save when only using Color 1-4:

\medskip
{
	\small
\newcolumntype{a}{>{\columncolor{pairedOneLightBlue}}c}
\newcolumntype{b}{>{\columncolor{pairedTwoDarkBlue}}c}
\newcolumntype{d}{>{\columncolor{pairedThreeLightGreen}}c}
\newcolumntype{e}{>{\columncolor{pairedFourDarkGreen}}c}
\newcolumntype{f}{>{\columncolor{pairedFiveLightRed}}c}
\newcolumntype{g}{>{\columncolor{pairedSixDarkRed}}c}

\begin{tabular}{a b d e f g}
Color 1 & Color 2 & Color 3 & Color 4 & Color 5 & Color 6\\
\#a6cee3 & \#1f78b4 & \#b2df8a & \#33a02c & \#fb9a99 & \#e31a1c
\end{tabular}
}

We de-emphasize components in figures by using additionally two shades of gray.
Especially in complex figures, it is often helpful to de-emphasize visual
elements that we want to represent but that should not be the focus of a
reader's attention.

\medskip
{
	\small
\newcolumntype{h}{>{\columncolor{pairedNegOneLightGray}}c}
\newcolumntype{i}{>{\columncolor{pairedNegTwoDarkGray}}c}

\begin{tabular}{h i}
Color -1 & Color -2\\
\#cacaca & \#827b7b\\
\end{tabular}
}

Single-color graphs are plotted in Color 1 - Light Blue.

\paragraph{Labels in Figures}
Complex diagrams often benefit from labels inside the diagrams. We suggest to
use a filled circle (e.g, in light blue) to highlight these numbers and use
these references, e.g., \circled{1} implemented as \texttt{\textbackslash{}circled\{1\}}, in the text to refer to them.

\paragraph{Captions and Core Message}
\label{appendix:captions}

Each figure should have a caption that makes a clear statement about this
figure, as such a statement makes it easier for the reader to (in)validate the
figure as evidence for the claim we make. Traditionally, figures often have a
caption indicating its content:\\ {\color{pairedTwoDarkBlue} \textit{$\cdot$
Speedup of approach A vs approach B on system X}}\\ {\color{pairedTwoDarkBlue}
\textit{$\cdot$ Architecture diagram of our solution}}\\ While these statements
clearly state the content of a figure at the meta-level, they often lack
information about the precise content and the claim a figure is meant to
evidence. While knowing that a figure is an architecture diagram is useful for
the reader when looking at the figure the reader automatically asks two
questions: (a) what properties set this architecture apart and (b) does its
implementation deliver the claimed properties? Or, in more general terms, what
claims do we aim to evidence with this figure and does the figure provide the
needed evidence to support our claims? In theory, this information could be
contained in the text of the paper, but to optimize for readers who skim the
figures first, we want to offer them as part of the caption. Nevertheless, it
makes often sense to word the caption strategically to still document the
meta-level content of a figure.\\ $\to$ {\color{pairedFourDarkGreen}
\textit{$\cdot$ Approach A is consistently faster than approach B, except for
inputs that are not used in practice}}\\ $\to$ {\color{pairedFourDarkGreen}
\textit{$\cdot$ The architecture of our design increases reusability by making
components A, B, \& C independent of the core.}} For example, after reading the
last caption the reader can validate if the architecture design indeed enables
the promised independence, and we can double-check while drafting the paper that
our figure is visualized to facilitate checking if it works as evidence. ! This
does not mean we should mislead with our figure but rather make things easy to
check. If our figure or data would not support our claim, it should be similarly
easy to invalidate our claim!

\subsubsection{Plots} We use matplotlib to create performance
plots such as \autoref{fig:speedup}. We use the following
formatting guidelines:
\begin{itemize}
  \item Use a vertical y-label to make it easier to read.
  \item Remove top and right frames to reduce visual noise
	and allow the reader to focus on the data in the
	figure.
  \item Provide the concrete data at the top of each bar.
\end{itemize}

\noindent
We also suggest to follow these technical remarks:
\begin{itemize}
  \item Create pdf plots and do not use bitmap formats (e.g., png) to
	ensure high quality when zooming in.
  \item Avoid Type-3 bitmap fonts by
	setting fonttype to 42.
\end{itemize}

\begin{figure}
\includegraphics[width=\columnwidth]{plots/speedup}
\caption{Improved running speed after 4 weeks of training.
}
\label{fig:speedup}
\end{figure}

\subsubsection{Tables} We optimize our tables for readability by removing as
much clutter as possible, while highlighting the key structure. Markus Püschel
(see doc/paper-writing/guide-tables.pdf) wrote a nice guide on how to make nice
tables. \autoref{tab:simple_table} illustrates this with a simple
table.

\begin{table}
\ra{1.2}
\centering
\begin{tabular}{l l l r}
  \toprule
  \textbf{Animal} & \textbf{Size} & \textbf{Biotope} & \textbf{Age}\\
  \midrule
  Dog & Medium & Ground & 20\\
  Cat & Medium & Ground &20 \\
  Ant & Small & Ground & 30 \\
  Elephant & Large & Ground & 70\\
  Whale & Large & Water & 100\\
  Salmon & Medium & Water & 13 \\
  Eagle & Large & Air & 35 \\
  \bottomrule
\end{tabular}
\vspace{1em}
\caption{A table with heigh lines and emphasized header.}
\label{tab:simple_table}
\end{table}

\subsubsection{Listings} We aim to use minted to create listings as much as
possible, as this allows us to edit code quickly. We use syntax highlighting
to make the parts of the code that matter most stand out. Hence, we keep
most code black, comments gray, and highlight just the MLIR operands that
we care about most.

\begin{listing}[H]
% We cannot put '{' on a line after % in draftonly mode, as the hack we used to
% not include the draft section will interpret the listing as normal
% latex where '%' is a comment and {} need to match, which they will
% not if only one is commented.
\begin{mlir}
// This is a comment
def @foo(%0 : !dialect.type)
{
  %a = dialect.op(%0) : !dialect.type // $\color{black}\circled{a}$
}
\end{mlir}
\caption{A simple MLIR code example with markers. Markers can also be placed in
	captions and refer to labels, e.g. \circled[lst:example]{a}.}
\label{lst:example}
\end{listing}

\begin{listing}[H]
\begin{lean4}
theorem funext {f₁ f₂ : ∀ (x : α), β x}
  (h : ∀ x, f₁ x = f₂ x) : f₁ = f₂ := by
  show extfunApp (Quotient.mk' f₁) = 
       extfunApp (Quotient.mk' f₂)
  apply congrArg
  apply Quotient.sound
  exact h
\end{lean4}
\caption{A simple Lean4 code example, taken from
  \url{https://lean-lang.org/lean4/doc/syntax\_highlight\_in\_latex.html\#example-with-minted}.}
\end{listing}

The syntax highlighting also works for xDSL-like IRs. Notice that different
minted styles can be used for different environments. The xDSL environment uses
the murphy-style in this case, whereas the MLIR version applies the
colorful-style.

\begin{xdsl}
func.func() [sym_name = "main", function_type = !fun<[
              !iterators.columnar_batch<!tuple<[!i64]>>
                 ], []>] {
  ^bb0(%0 : !iterators.columnar_batch<!tuple<[!i64]>>):
    %t : !iterators.stream<!llvm.struct<[!i64]>> =
      iterators.scan_columnar_batch(%0 : ...)
    %filtered : !iterators.stream<!llvm.struct<[!i64]>> =
          iterators.filter(%input : …) [predicateRef = @s0]
    iterators.sink(%filtered : !iterators.stream<!tuple<[!i64]>>)
    func.return()
}

func.func() [sym_name = "s0", function_type = !fun<[
    !llvm.struct<[!i64]>], [!i1]>] {
  ^bb0(%struct : !llvm.struct<[!i64]>):
    %id : !i64 = llvm.extractvalue(%struct : ...)
              [position = [0 : !index]]
    %five  : !i64 = arith.constant() [value = 5 : !i64]
    %cmp : !i1 = arith.cmpi(%id : !i64, %five : !i64)
              [predicate = 4 : !i64]
    func.return(%cmp : !i1)
}
\end{xdsl}

\section{Writing}

A couple of hints with respect to how we write text.

\subsection{Citations}
\label{appendix:citations}

\subsubsection{Do not use numerical citations as nouns}
Especially when working with numerical citations (e.g., [1]) the use of
citations as nouns reduces readability. Hence, we do not use numerical citations
as nouns and instead expand these citations with \texttt{\textbackslash{}citet} to the
authornames.
\\
{\color{pairedTwoDarkBlue}
\textit{[1] showed that .. $\dots$}}\\
$\to$ {\color{pairedFourDarkGreen}\textit{Author et al. [1] showed}}

\subsubsection{Prefer meaningful text over citations as textual content}
While acknowledging authors of work is important, maximizing the amount of technical
content (outside of a historic perspective) typically makes text more direct and
concrete. Hence, we avoid the discussion of who did what in text if the
historic context does not add meaning or empty words can be replaced by an immediate
citation. E.g, in the following the words `introduced in` are not carrying
information and can be dropped.\\
{\color{pairedTwoDarkBlue}
\textit{we extend PreviousIdea introduced in [1] $\dots$ by}}\\
$\to$ {\color{pairedFourDarkGreen}\textit{we extend PreviousIdea [1] by}}


\subsubsection{Managing acronyms automatically}
Managing acronyms manually can lead to situations where the specific term is not properly expanded upon first use or when it is introduced.
The \texttt{acronym} package is useful to avoid such situations and provides full control over acronyms.
For example, assume we have defined an acronym with \texttt{\textbackslash{}newacronym\{ir\}\{IR\}\{Intermediate Representation\}}:
\begin{itemize}
  \item Upon first use of \texttt{\textbackslash{}ac\{ir\}} we get: \ac{ir}.
  \item On the second reference: \ac{ir}.
  \item If we need to force expansion for a figure or a background section where the term is first described, we can use \texttt{\textbackslash{}acf\{ir\}} which gives: \acf{ir}.
  \item We can obtain plural form using \texttt{\textbackslash{}acp\{ir\}} giving: \acp{ir}.
\end{itemize}

\end{draftonly}


\end{document}
